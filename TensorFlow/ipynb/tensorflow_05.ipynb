{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : TensorFlow Machine Learning Cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 계산 그래프를 통해 오차를 역전파해서 변수 값을 갱신하고 비용 함수 값을 최소화하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #25 A = [6.158196]\n",
      "Loss = [12.751575]\n",
      "\n",
      "Step #50 A = [8.553392]\n",
      "Loss = [2.9105444]\n",
      "\n",
      "Step #75 A = [9.590639]\n",
      "Loss = [0.02027335]\n",
      "\n",
      "Step #100 A = [9.812081]\n",
      "Loss = [0.10290702]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 및 플레이스홀더 생성\n",
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1]))\n",
    "\n",
    "# 그래프에 곱하기 연산 선언\n",
    "my_output = tf.multiply(x_data, A)\n",
    "\n",
    "# L2 비용 함수 선언\n",
    "loss = tf.square(my_output - y_target)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# 그래프 변수 최적화 방식 선언\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.02)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# 비용 함수 값 출력\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data:rand_x, y_target:rand_y})\n",
    "    \n",
    "    if (i+1) % 25 == 0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(loss, feed_dict={x_data:rand_x, y_target:rand_y})) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 A = [6.5119305]\n",
      "Loss = [[5.5987304e-05]]\n",
      "\n",
      "Step #400 A = [2.1868773]\n",
      "Loss = [[1.628742]]\n",
      "\n",
      "Step #600 A = [-0.08982835]\n",
      "Loss = [[0.4642579]]\n",
      "\n",
      "Step #800 A = [-0.6914349]\n",
      "Loss = [[0.46073717]]\n",
      "\n",
      "Step #1000 A = [-0.76806545]\n",
      "Loss = [[0.35771972]]\n",
      "\n",
      "Step #1200 A = [-0.73560107]\n",
      "Loss = [[0.17253962]]\n",
      "\n",
      "Step #1400 A = [-0.80941415]\n",
      "Loss = [[0.1717363]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 및 플레이스홀더 생성\n",
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(mean=10, shape=[1]))\n",
    "\n",
    "# 그래프에 평행 이동 연산 선언\n",
    "my_output = tf.add(x_data, A)\n",
    "\n",
    "# 결과 값에 차원 추가\n",
    "my_output_expanded = tf.expand_dims(my_output, 0)\n",
    "y_target_expanded = tf.expand_dims(y_target, 0)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# 비용 함수 선언\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output_expanded, labels=y_target_expanded)\n",
    "\n",
    "# 그래프 변수 최적화 방식 선언\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(xentropy)\n",
    "\n",
    "# 비용 함수 값 출력\n",
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data:rand_x, y_target:rand_y})\n",
    "    \n",
    "    if (i+1) % 200 == 0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data:rand_x, y_target:rand_y})) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "< 학습률 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<table>\n",
    "    <tr> <td>좀 더 작은 학습률</td> <td>수렴이 더 느리지만, 결과가 더 정확함</td> <td>안정적인 답이 나오지 않으면 학습률을 낮추어 본다</td> </tr>\n",
    "    <tr> <td>좀 더 큰 학습률</td> <td>결과가 덜 정확하지만, 더 빨리 수렴됨</td> <td>경우에 따라 답이 정체에 빠지는 문제를 해결해 줄 수 있다</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "< 최적화 알고리즘 >\n",
    "* tf.train.GradientDescentOptimizer()\n",
    "* tf.train.MomentumOptimizer()\n",
    "* tf.train.AdagradOptimizer()\n",
    "* tf.train.AdadeltaOptimizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
